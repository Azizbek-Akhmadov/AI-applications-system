{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAs47SxrZBNKuueANlu0K+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azizbek-Akhmadov/Mid-term-exam-AI-applications-system/blob/main/Ai_application_week_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tS34YVsUwPlf",
        "outputId": "84ae2e60-8f34-46bf-e475-3684104e3834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.49.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.0.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 85.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Collecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 64.8 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.1.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=830a64f47d48f13699093b8037af70918890810c622f2b6ed2400b60d5a01aea\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "h5py",
                  "numpy",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install tensorflow==1.15.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "N455PyX1wZha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_parameter = 0.01\n",
        "epochs = 300"
      ],
      "metadata": {
        "id": "WbxK6oAKwbob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_points = 50\n",
        "x_train = np.linspace(0,30,sample_points)\n",
        "y_train = 6*x_train + 7*np.random.randn(sample_points)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "oaNCvMRawekZ",
        "outputId": "b89f362f-ec8f-4e18-ccd5-1ea27d46c023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b7149838aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noisy dataset\n",
        "plt.plot(x_train, y_train, 'o')\n",
        "# Noise free dataset \n",
        "plt.plot(x_train, 6*x_train)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "8JZJyFBVwgw_",
        "outputId": "7ab794e7-882a-47ff-b7f5-1cf3530e6d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a7a82eb5c858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Noisy dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Noise free dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = tf.placeholder(tf.float32)\n",
        "X = tf.placeholder(tf.float32)\n",
        "\n",
        "W = tf.Variable(np.random.randn(), name = 'weights')\n",
        "B = tf.Variable(np.random.randn(), name = 'bias')"
      ],
      "metadata": {
        "id": "TMKS1W_qwj-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model for regression\n",
        "prediction = W*X + B\n",
        "\n",
        "# Cost function\n",
        "cost_iteration = tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "\n",
        "#Define the optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "N3OA_VtNwqQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      sess.run(optimizer, feed_dict = {X : x, Y : y})\n",
        "    if not epoch%40:\n",
        "      W1 = sess.run(W)\n",
        "      B1 = sess.run(B)\n",
        "      cost_iter = sess.run(cost_iteration, feed_dict = {X : x, Y : y})\n",
        "      print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight = sess.run(W)\n",
        "  Bias = sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train,Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8x3UejNMwrT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model for regression\n",
        "with tf.name_scope(\"Model\") as scope:\n",
        "  prediction = W*X + B\n",
        "\n",
        "# Add summary to study behaviour of weights and biases with epochs\n",
        "weight_histogram = tf.summary.histogram(\"Weights\", W)\n",
        "bias_histogram = tf.summary.histogram(\"Bias\", B)\n",
        "\n",
        "# Cost function\n",
        "with tf.name_scope(\"Cost_function\") as scope:\n",
        "  cost_iteration = tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "\n",
        "# Record the scalar summary of the cost function\n",
        "cost_summary = tf.summary.scalar(\"Cost\", cost_iteration)"
      ],
      "metadata": {
        "id": "to0A1io3wwhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the optimizer\n",
        "with tf.name_scope(\"Training\") as scope:\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Merge all the summaries into a single operator\n",
        "merged_summary = tf.summary.merge_all()"
      ],
      "metadata": {
        "id": "-zO9azpgwypi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tensorflow session\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  writer = tf.summary.FileWriter('./log', sess.graph)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      sess.run(optimizer, feed_dict = {X : x, Y : y})\n",
        "\n",
        "      # Write logs for each epochs\n",
        "      summary_epochs = sess.run(merged_summary, feed_dict = {X : x, Y : y})\n",
        "      writer.add_summary(summary_epochs, epoch)\n",
        "    if not epoch%40:\n",
        "      W1 = sess.run(W)\n",
        "      B1 = sess.run(B)\n",
        "      cost_iter = sess.run(cost_iteration, feed_dict = {X : x, Y : y})\n",
        "      print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight = sess.run(W)\n",
        "  Bias = sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train,Weight*x_train+Bias)\n",
        "  plt.show("
      ],
      "metadata": {
        "id": "8x6yCY9tw0tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "id": "C4VGJC3_w3C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-freebsd-amd64.tgz"
      ],
      "metadata": {
        "id": "xs3cGja-w5vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ngrok-v3-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "6HYSxx34w87t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "metadata": {
        "id": "XZE3-MvZw_BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "metadata": {
        "id": "6PsmzFITxAoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "id": "SzXy59EyxCVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "id": "VnIt-az5xE8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dependencies\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "3QRu5VfwxHZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets(\"./data\", one_hot = True)"
      ],
      "metadata": {
        "id": "HdxtjGWLxJZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training = mnist_data.train.num_examples\n",
        "num_testing = mnist_data.test.num_examples\n",
        "num_validation = mnist_data.validation.num_examples\n",
        "print(\"MNIST Datasize: Training samples: {0}, Testing samples: {1}\")"
      ],
      "metadata": {
        "id": "ct1dZLrdxL_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network parameters of Neural Network\n",
        "n_input = 784     # Input image of size 28 x 28\n",
        "n_hidden_1 = 512  # First hidden layer\n",
        "n_hidden_2 = 256  # Second hidden layer\n",
        "n_hidden_3 = 128  # Third hidden layer\n",
        "n_output = 10     # Output layer having (0-9) digits"
      ],
      "metadata": {
        "id": "BKV1MSaexOQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "epochs = 3000\n",
        "batch_size = 128\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "metadata": {
        "id": "x2rltwflxQJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building tensorflow graph\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_input])\n",
        "Y = tf.placeholder(tf.float32, [None, n_output]"
      ],
      "metadata": {
        "id": "z98WITwtxRvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_weight = {\"W1\": tf.Variable(tf.truncated_normal([n_input, n_hidden_1], stddev = 0.1)),\n",
        "             \"W2\": tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev = 0.1)),\n",
        "             \"W3\": tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], stddev = 0.1)),\n",
        "             \"Wout\":tf.Variable(tf.truncated_normal([n_hidden_3, n_output]))\n",
        "}\n",
        "\n",
        "nn_bias = { \"B1\": tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "            \"B2\": tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "            \"B3\": tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "            \"B4\": tf.Variable(tf.truncated_normal([n_output])),  \n",
        "           }"
      ],
      "metadata": {
        "id": "-3IuErq9xTOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_layer_1 = tf.add(tf.matmul(X, nn_weight[\"W1\"]),nn_bias[\"B1\"])\n",
        "nn_layer_2 = tf.add(tf.matmul(nn_layer_1, nn_weight[\"W2\"]),nn_bias[\"B2\"])\n",
        "nn_layer_3 = tf.add(tf.matmul(nn_layer_2, nn_weight[\"W3\"]),nn_bias[\"B3\"])\n",
        "layer_drop = tf.nn.dropout(nn_layer_3, keep_prob)\n",
        "output_layer = tf.add(tf.matmul(layer_drop, nn_weight[\"Wout\"]), nn_bias[\"B4\"])"
      ],
      "metadata": {
        "id": "rgc97lMVxVVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss\n",
        "computed_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = output_layer, labels = Y))\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(computed_loss)\n",
        "\n",
        "#Define prediction\n",
        "prediction_out = tf.equal(tf.argmax(output_layer,1), tf.argmax(Y,1))\n",
        "\n",
        "# Define accuracy of the model\n",
        "nn_accuracy = tf.reduce_mean(tf.cast(prediction_out, tf.float32))\n",
        "\n",
        "# Initialize all the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "metadata": {
        "id": "Ra8_0BX6xW5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the computational graph\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(epochs):\n",
        "\n",
        "    mini_batch_x, mini_batch_y = mnist_data.train.next_batch(batch_size)\n",
        "    #print(mini_batch_x[0:1,:].shape)\n",
        "    mini_batch_val_x, mini_batch_val_y = mnist_data.validation.next_batch(batch_size)\n",
        "\n",
        "    sess.run(optimizer, feed_dict = {X : mini_batch_x, Y : mini_batch_y, keep_prob:1})\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      mini_batch_loss, mini_batch_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      mini_batch_val_loss, mini_batch_val_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      print(\"Iterations : {0} , Train_loss = {1}, Train_Accuracy {2}, Val_loss {3}, Val_accuracy {4}\".format(i, mini_batch_loss, mini_batch_accuracy, mini_batch_val_loss, mini_batch_val_accuracy))\n",
        "\n",
        "  print(\"Optimization Finished\")\n",
        "  test_accuracy = sess.run(nn_accuracy, feed_dict = {X:mnist_data.test.images, Y:mnist_data.test.labels, keep_prob:1.0})\n",
        "  print(\"Testing accuracy is {0}\".format(test_accuracy))\n",
        "\n",
        "  saver_path = saver.save(sess, \"./model/my_model.ckpt\")"
      ],
      "metadata": {
        "id": "1Y7khQgKxZBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}